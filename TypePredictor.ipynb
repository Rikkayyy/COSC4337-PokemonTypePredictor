{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7060 - accuracy: 0.5037 - val_loss: 0.6941 - val_accuracy: 0.4878\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4576 - accuracy: 0.8695 - val_loss: 0.8154 - val_accuracy: 0.5122\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9817 - val_loss: 0.8946 - val_accuracy: 0.5024\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9988 - val_loss: 1.1336 - val_accuracy: 0.5171\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9988 - val_loss: 1.1258 - val_accuracy: 0.4829\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.1945 - val_accuracy: 0.4976\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.3075 - val_accuracy: 0.5073\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3603 - val_accuracy: 0.5220\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3992 - val_accuracy: 0.5220\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.4319 - val_accuracy: 0.5268\n",
      "7/7 [==============================] - 0s 905us/step - loss: 1.4319 - accuracy: 0.5268\n",
      "Test Accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. Load and preprocess data\n",
    "df = pd.read_csv('pokedex_with_type_count.csv')\n",
    "\n",
    "# Drop missing or invalid info entries\n",
    "df = df.dropna(subset=['info', 'type_count'])\n",
    "\n",
    "# 2. Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['info'])\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "sequences = tokenizer.texts_to_sequences(df['info'])\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 3. Encode labels: 'Mono' -> 0, 'Dual' -> 1\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['type_count'])\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_len))\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# 6. Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 7. Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 8. Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 1s 15ms/step - loss: 0.5844 - accuracy: 0.0152 - val_loss: 0.3580 - val_accuracy: 0.0244\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.3107 - accuracy: 0.0076 - val_loss: 0.2847 - val_accuracy: 0.0549\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.2870 - accuracy: 0.0777 - val_loss: 0.2825 - val_accuracy: 0.0549\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.2860 - accuracy: 0.0777 - val_loss: 0.2827 - val_accuracy: 0.0549\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.2858 - accuracy: 0.0777 - val_loss: 0.2824 - val_accuracy: 0.0549\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.2859 - accuracy: 0.0777 - val_loss: 0.2826 - val_accuracy: 0.0549\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.2861 - accuracy: 0.0777 - val_loss: 0.2824 - val_accuracy: 0.0549\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.2860 - accuracy: 0.0777 - val_loss: 0.2826 - val_accuracy: 0.0549\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.2859 - accuracy: 0.0777 - val_loss: 0.2825 - val_accuracy: 0.0549\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.2857 - accuracy: 0.0777 - val_loss: 0.2827 - val_accuracy: 0.0549\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "Per-type accuracy:\n",
      "                  0\n",
      "bug       0.912195\n",
      "dark      0.926829\n",
      "dragon    0.936585\n",
      "electric  0.921951\n",
      "fairy     0.946341\n",
      "fighting  0.912195\n",
      "fire      0.941463\n",
      "flying    0.921951\n",
      "ghost     0.931707\n",
      "grass     0.873171\n",
      "ground    0.921951\n",
      "ice       0.965854\n",
      "normal    0.887805\n",
      "poison    0.882927\n",
      "psychic   0.892683\n",
      "rock      0.931707\n",
      "steel     0.936585\n",
      "water     0.858537\n",
      "\n",
      "Correlation matrix between types:\n",
      "      0\n",
      "0  1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('pokedex_with_type_count.csv')\n",
    "df = df.dropna(subset=['info', 'type'])\n",
    "\n",
    "# Convert type string into list: \"{Fire,Water}\" -> ['Fire', 'Water']\n",
    "df['type_list'] = df['type'].str.strip('{}').str.split(',')\n",
    "\n",
    "# Use MultiLabelBinarizer for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['type_list'])  # Each column is a type\n",
    "\n",
    "# Tokenize the 'info' column\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['info'])\n",
    "sequences = tokenizer.texts_to_sequences(df['info'])\n",
    "max_len = max(len(s) for s in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_len))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(len(mlb.classes_), activation='sigmoid'))  # multi-label\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  # convert probabilities to binary\n",
    "\n",
    "# Build per-type accuracy array\n",
    "per_type_accuracy = []\n",
    "\n",
    "for i, type_name in enumerate(mlb.classes_):\n",
    "    acc = accuracy_score(y_test[:, i], y_pred_binary[:, i])\n",
    "    per_type_accuracy.append(acc)\n",
    "\n",
    "# Convert to DataFrame for correlation analysis\n",
    "type_acc_df = pd.DataFrame([per_type_accuracy], columns=mlb.classes_)\n",
    "\n",
    "# Create correlation matrix (although it's just one row; this works best if you had multiple models/runs)\n",
    "corr_matrix = type_acc_df.T.corr()\n",
    "\n",
    "# Save or display\n",
    "print(\"Per-type accuracy:\\n\", type_acc_df.T)\n",
    "print(\"\\nCorrelation matrix between types:\\n\", corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/datascience/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "52/52 [==============================] - 84s 1s/step - loss: 0.3729 - accuracy: 0.0549 - val_loss: 0.3031 - val_accuracy: 0.0488\n",
      "Epoch 2/5\n",
      "52/52 [==============================] - 78s 2s/step - loss: 0.2780 - accuracy: 0.0963 - val_loss: 0.3004 - val_accuracy: 0.1024\n",
      "Epoch 3/5\n",
      "52/52 [==============================] - 79s 2s/step - loss: 0.2637 - accuracy: 0.1902 - val_loss: 0.2890 - val_accuracy: 0.1366\n",
      "Epoch 4/5\n",
      "52/52 [==============================] - 81s 2s/step - loss: 0.2399 - accuracy: 0.3098 - val_loss: 0.2773 - val_accuracy: 0.2390\n",
      "Epoch 5/5\n",
      "52/52 [==============================] - 80s 2s/step - loss: 0.2094 - accuracy: 0.4390 - val_loss: 0.2734 - val_accuracy: 0.2000\n",
      "7/7 [==============================] - 6s 724ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# BERT Model\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv('pokedex_with_type_count.csv')\n",
    "df = df.dropna(subset=['info', 'type'])\n",
    "\n",
    "# Process type strings like \"{Fire,Water}\" → ['Fire', 'Water']\n",
    "df['type_list'] = df['type'].str.strip('{}').str.split(',')\n",
    "\n",
    "# Encode labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['type_list'])\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize 'info' column\n",
    "tokens = tokenizer(\n",
    "    list(df['info']), \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train = {\n",
    "    'input_ids': tokens['input_ids'][:int(0.8 * len(df))],\n",
    "    'attention_mask': tokens['attention_mask'][:int(0.8 * len(df))]\n",
    "}\n",
    "X_test = {\n",
    "    'input_ids': tokens['input_ids'][int(0.8 * len(df)):],\n",
    "    'attention_mask': tokens['attention_mask'][int(0.8 * len(df)):]\n",
    "}\n",
    "y_train = y[:int(0.8 * len(df))]\n",
    "y_test = y[int(0.8 * len(df)):]\n",
    "\n",
    "# Load pre-trained BERT base model\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Freeze BERT layers (optional, for faster training)\n",
    "# for layer in bert.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# Build model\n",
    "input_ids = tf.keras.Input(shape=(tokens['input_ids'].shape[1],), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.Input(shape=(tokens['attention_mask'].shape[1],), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "embedding = bert(input_ids, attention_mask=attention_mask)[1]  # [1] gives the pooled output\n",
    "output = tf.keras.layers.Dense(len(mlb.classes_), activation='sigmoid')(embedding)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=16)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Optional: Save model\n",
    "# model.save('bert_pokemon_type_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
